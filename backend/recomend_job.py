from elasticsearch import Elasticsearch
import numpy as np
import json

# Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
es = Elasticsearch("http://elasticsearch:9200")
INDEX_NAME = "job_info"

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
dami_input = {
    "labels": [
        { "id": "365", "label": "good" },
        { "id": "628", "label": "good" },
        { "id": "395", "label": "good" },
        { "id": "665", "label": "good" },
        { "id": "599", "label": "good" },
        { "id": "574", "label": "good" },
        { "id": "389", "label": "good" },
        { "id": "8", "label": "bad" },
        { "id": "9", "label": "bad" },
        { "id": "10", "label": "bad" }
    ],
    "user_filter_label": False,
    "category_value": ""
}

# === ãƒ©ãƒ™ãƒ«åˆ†é¡ ===
good_ids = [entry["id"] for entry in dami_input["labels"] if entry["label"] == "good"]
bad_ids = [entry["id"] for entry in dami_input["labels"] if entry["label"] == "bad"]

# === ãƒ™ã‚¯ãƒˆãƒ«å–å¾— ===
def fetch_vectors(ids):
    vectors = []
    for doc_id in ids:
        try:
            res = es.get(index=INDEX_NAME, id=doc_id)
            vec = res["_source"].get("embedding")
            if vec:
                vectors.append(np.array(vec))
        except Exception as e:
            print(f"Error fetching {doc_id}: {e}")
    return vectors

good_vecs = fetch_vectors(good_ids)
bad_vecs = fetch_vectors(bad_ids)

mean_good = np.mean(good_vecs, axis=0) if good_vecs else None
mean_bad = np.mean(bad_vecs, axis=0) if bad_vecs else None

# === KNNæ¤œç´¢ ===
def knn_search(query_vec, k=70, num_candidates=600):
    query = {
        "knn": {
            "field": "embedding",
            "query_vector": query_vec.tolist(),
            "k": k,
            "num_candidates": num_candidates
        },
        "size": k
    }
    return es.search(index=INDEX_NAME, body=query)["hits"]["hits"]

good_hits = knn_search(mean_good)
bad_hits = knn_search(mean_bad)
print(f"\nğŸ” KNNæ¤œç´¢çµæœä»¶æ•°ï¼ˆmean_goodï¼‰: {len(good_hits)}")

# === Step 1: badã¨åŒä¸€ID/jobã‚’é™¤å¤– ===
bad_ids_set = set(hit["_id"] for hit in bad_hits)
bad_jobs_set = set(hit["_source"].get("job", "") for hit in bad_hits)

filtered_hits = []
for hit in good_hits:
    if (hit["_id"] not in bad_ids_set) and (hit["_source"].get("job", "") not in bad_jobs_set):
        filtered_hits.append(hit)
print(f"âœ… badé™¤å¤–å¾Œ: {len(filtered_hits)}")

# === Step 2: ã‚«ãƒ†ã‚´ãƒªé™¤å¤– ===
def filter_by_category(hits, category_value):
    filtered = []
    for hit in hits:
        industry = hit["_source"].get("job_tag", {}).get("industry", {})
        parent = industry.get("parent", "")
        child = industry.get("child", "")
        # category_valueãŒè¦ªã‚«ãƒ†ã‚´ãƒªã¾ãŸã¯å­ã‚«ãƒ†ã‚´ãƒªã¨ä¸€è‡´ã—ãªã„å ´åˆã«å«ã‚ã‚‹
        if category_value != parent and category_value != child:
            filtered.append(hit)
    return filtered

# user_filter_labelãŒTrueã®å ´åˆã®ã¿ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
if dami_input.get("user_filter_label"): # Trueã®å ´åˆã«å®Ÿè¡Œ
    filtered_hits = filter_by_category(filtered_hits, dami_input["category_value"])
    print(f"âœ… ã‚«ãƒ†ã‚´ãƒªé™¤å¤–å¾Œï¼ˆ{dami_input['category_value']}ï¼‰: {len(filtered_hits)}")
else: # Falseã®å ´åˆï¼ˆã¾ãŸã¯ã‚­ãƒ¼ãŒãªã„å ´åˆï¼‰ã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—
    print("â˜‘ ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")

# === Step 3: childã‚«ãƒ†ã‚´ãƒªãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«æŠ½å‡º ===
def select_diverse_hits(hits, max_results=10):
    seen_childs = set()
    diverse_hits = []
    for hit in hits:
        child = hit["_source"].get("job_tag", {}).get("industry", {}).get("child", "")
        if child not in seen_childs:
            diverse_hits.append(hit)
            seen_childs.add(child)
        if len(diverse_hits) >= max_results:
            break
    return diverse_hits

diverse_top_hits = select_diverse_hits(filtered_hits, max_results=10)

# === è©³ç´°å–å¾—ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ä»¥å¤–ï¼‰ ===
def fetch_job_details(ids):
    jobs = []
    for doc_id in ids:
        try:
            res = es.get(index=INDEX_NAME, id=doc_id)
            source = res["_source"].copy()
            source["id"] = doc_id
            source.pop("embedding", None)
            jobs.append(source)
        except Exception as e:
            print(f"Error fetching job details for {doc_id}: {e}")
    return jobs

good_job_details = fetch_job_details(good_ids)
bad_job_details = fetch_job_details(bad_ids)

recommended_jobs = []
for hit in diverse_top_hits:
    job = hit["_source"].copy()
    job["id"] = hit["_id"]
    job.pop("embedding", None)
    job["score"] = hit["_score"]
    recommended_jobs.append(job)

# === æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ ===
response = {
    "good_jobs": good_job_details,
    "bad_jobs": bad_job_details,
    "recommended_jobs": recommended_jobs
}

# === è¡¨ç¤ºç”¨å‡ºåŠ› ===
print("\nâœ… GOOD ãƒ©ãƒ™ãƒ«ä»˜ãæ±‚äºº:")
for job in good_job_details:
    print(f"ID: {job.get('id')} / Job: {job.get('job')} / Parent: {job.get('job_tag', {}).get('industry', {}).get('parent')} / Child: {job.get('job_tag', {}).get('industry', {}).get('child')}")

print("\nâŒ BAD ãƒ©ãƒ™ãƒ«ä»˜ãæ±‚äºº:")
for job in bad_job_details:
    print(f"ID: {job.get('id')} / Job: {job.get('job')} / Parent: {job.get('job_tag', {}).get('industry', {}).get('parent')} / Child: {job.get('job_tag', {}).get('industry', {}).get('child')}")

print("\nâ­ RECOMMENDED JOBSï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸Šä½ãƒ»å¤šæ§˜æ€§ã‚ã‚Šï¼‰:")
for job in recommended_jobs:
    print(f"ID: {job.get('id')} / Job: {job.get('job')} / Child: {job.get('job_tag', {}).get('industry', {}).get('child')} / Score: {job.get('score'):.4f}")
